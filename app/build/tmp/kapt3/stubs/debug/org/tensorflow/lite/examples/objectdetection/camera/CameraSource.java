package org.tensorflow.lite.examples.objectdetection.camera;

/**
 * Manages the camera and allows UI updates on top of it (e.g. overlaying extra Graphics). This
 * receives preview frames from the camera at a specified rate, sends those frames to detector as
 * fast as it is able to process.
 *
 *
 * This camera source makes a best effort to manage processing on preview frames as fast as
 * possible, while at the same time minimizing lag. As such, frames may be dropped if the detector
 * is unable to keep up with the rate of frames generated by the camera.
 */
@kotlin.Metadata(mv = {1, 9, 0}, k = 1, xi = 48, d1 = {"\u0000t\n\u0002\u0018\u0002\n\u0002\u0010\u0000\n\u0000\n\u0002\u0018\u0002\n\u0002\b\u0002\n\u0002\u0018\u0002\n\u0002\u0010\u0012\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0002\b\u0004\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0010\b\n\u0002\b\u0003\n\u0002\u0010\u0002\n\u0002\b\u0004\n\u0002\u0018\u0002\n\u0002\b\u0003\n\u0002\u0018\u0002\n\u0002\b\u0005\n\u0002\u0010\u000e\n\u0002\b\u0003\u0018\u0000 /2\u00020\u0001:\u0002/0B\r\u0012\u0006\u0010\u0002\u001a\u00020\u0003\u00a2\u0006\u0002\u0010\u0004J\b\u0010\u001c\u001a\u00020\nH\u0002J\u0010\u0010\u001d\u001a\u00020\u00072\u0006\u0010\u0011\u001a\u00020\u0010H\u0002J\u0006\u0010\u001e\u001a\u00020\u001fJ\u000e\u0010 \u001a\u00020\u001f2\u0006\u0010!\u001a\u00020\u000eJ\u001c\u0010\"\u001a\u00020\u001f2\u0006\u0010\t\u001a\u00020\n2\n\u0010#\u001a\u00060$R\u00020\nH\u0002J\u001c\u0010%\u001a\u00020\u001f2\u0006\u0010\t\u001a\u00020\n2\n\u0010#\u001a\u00060$R\u00020\nH\u0002J\u0015\u0010&\u001a\u00020\u001f2\u0006\u0010\'\u001a\u00020(H\u0000\u00a2\u0006\u0002\b)J\r\u0010*\u001a\u00020\u001fH\u0000\u00a2\u0006\u0002\b+J\u000e\u0010,\u001a\u00020\u001f2\u0006\u0010-\u001a\u00020.R\u001a\u0010\u0005\u001a\u000e\u0012\u0004\u0012\u00020\u0007\u0012\u0004\u0012\u00020\b0\u0006X\u0082\u0004\u00a2\u0006\u0002\n\u0000R\u0010\u0010\t\u001a\u0004\u0018\u00010\nX\u0082\u000e\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u000b\u001a\u00020\fX\u0082\u0004\u00a2\u0006\u0002\n\u0000R\u0010\u0010\r\u001a\u0004\u0018\u00010\u000eX\u0082\u000e\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u0002\u001a\u00020\u0003X\u0082\u0004\u00a2\u0006\u0002\n\u0000R\"\u0010\u0011\u001a\u0004\u0018\u00010\u00102\b\u0010\u000f\u001a\u0004\u0018\u00010\u0010@BX\u0080\u000e\u00a2\u0006\b\n\u0000\u001a\u0004\b\u0012\u0010\u0013R\u0012\u0010\u0014\u001a\u00060\u0015R\u00020\u0000X\u0082\u0004\u00a2\u0006\u0002\n\u0000R\u0010\u0010\u0016\u001a\u0004\u0018\u00010\u0017X\u0082\u000e\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u0018\u001a\u00020\u0019X\u0082\u0004\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u001a\u001a\u00020\u001bX\u0082\u000e\u00a2\u0006\u0002\n\u0000\u00a8\u00061"}, d2 = {"Lorg/tensorflow/lite/examples/objectdetection/camera/CameraSource;", "", "graphicOverlay", "Lorg/tensorflow/lite/examples/objectdetection/camera/GraphicOverlay;", "(Lorg/tensorflow/lite/examples/objectdetection/camera/GraphicOverlay;)V", "bytesToByteBuffer", "Ljava/util/IdentityHashMap;", "", "Ljava/nio/ByteBuffer;", "camera", "Landroid/hardware/Camera;", "context", "Landroid/content/Context;", "frameProcessor", "Lorg/tensorflow/lite/examples/objectdetection/camera/FrameProcessor;", "<set-?>", "Lcom/google/android/gms/common/images/Size;", "previewSize", "getPreviewSize$app_debug", "()Lcom/google/android/gms/common/images/Size;", "processingRunnable", "Lorg/tensorflow/lite/examples/objectdetection/camera/CameraSource$FrameProcessingRunnable;", "processingThread", "Ljava/lang/Thread;", "processorLock", "Ljava/lang/Object;", "rotationDegrees", "", "createCamera", "createPreviewBuffer", "release", "", "setFrameProcessor", "processor", "setPreviewAndPictureSize", "parameters", "Landroid/hardware/Camera$Parameters;", "setRotation", "start", "surfaceHolder", "Landroid/view/SurfaceHolder;", "start$app_debug", "stop", "stop$app_debug", "updateFlashMode", "flashMode", "", "Companion", "FrameProcessingRunnable", "app_debug"})
@kotlin.Suppress(names = {"DEPRECATION"})
public final class CameraSource {
    @org.jetbrains.annotations.NotNull
    private final org.tensorflow.lite.examples.objectdetection.camera.GraphicOverlay graphicOverlay = null;
    @org.jetbrains.annotations.Nullable
    private android.hardware.Camera camera;
    private int rotationDegrees = 0;
    
    /**
     * Returns the preview size that is currently in use by the underlying camera.
     */
    @org.jetbrains.annotations.Nullable
    private com.google.android.gms.common.images.Size previewSize;
    
    /**
     * Dedicated thread and associated runnable for calling into the detector with frames, as the
     * frames become available from the camera.
     */
    @org.jetbrains.annotations.Nullable
    private java.lang.Thread processingThread;
    @org.jetbrains.annotations.NotNull
    private final org.tensorflow.lite.examples.objectdetection.camera.CameraSource.FrameProcessingRunnable processingRunnable = null;
    @org.jetbrains.annotations.NotNull
    private final java.lang.Object processorLock = null;
    @org.jetbrains.annotations.Nullable
    private org.tensorflow.lite.examples.objectdetection.camera.FrameProcessor frameProcessor;
    
    /**
     * Map to convert between a byte array, received from the camera, and its associated byte buffer.
     * We use byte buffers internally because this is a more efficient way to call into native code
     * later (avoids a potential copy).
     *
     *
     * **Note:** uses IdentityHashMap here instead of HashMap because the behavior of an array's
     * equals, hashCode and toString methods is both useless and unexpected. IdentityHashMap enforces
     * identity ('==') check on the keys.
     */
    @org.jetbrains.annotations.NotNull
    private final java.util.IdentityHashMap<byte[], java.nio.ByteBuffer> bytesToByteBuffer = null;
    @org.jetbrains.annotations.NotNull
    private final android.content.Context context = null;
    public static final int CAMERA_FACING_BACK = android.hardware.Camera.CameraInfo.CAMERA_FACING_BACK;
    @org.jetbrains.annotations.NotNull
    private static final java.lang.String TAG = "CameraSource";
    private static final int IMAGE_FORMAT = android.graphics.ImageFormat.NV21;
    private static final int MIN_CAMERA_PREVIEW_WIDTH = 400;
    private static final int MAX_CAMERA_PREVIEW_WIDTH = 1300;
    private static final int DEFAULT_REQUESTED_CAMERA_PREVIEW_WIDTH = 640;
    private static final int DEFAULT_REQUESTED_CAMERA_PREVIEW_HEIGHT = 360;
    private static final float REQUESTED_CAMERA_FPS = 30.0F;
    @org.jetbrains.annotations.NotNull
    public static final org.tensorflow.lite.examples.objectdetection.camera.CameraSource.Companion Companion = null;
    
    public CameraSource(@org.jetbrains.annotations.NotNull
    org.tensorflow.lite.examples.objectdetection.camera.GraphicOverlay graphicOverlay) {
        super();
    }
    
    /**
     * Returns the preview size that is currently in use by the underlying camera.
     */
    @org.jetbrains.annotations.Nullable
    public final com.google.android.gms.common.images.Size getPreviewSize$app_debug() {
        return null;
    }
    
    /**
     * Opens the camera and starts sending preview frames to the underlying detector. The supplied
     * surface holder is used for the preview so frames can be displayed to the user.
     *
     * @param surfaceHolder the surface holder to use for the preview frames.
     * @throws IOException if the supplied surface holder could not be used as the preview display.
     */
    @kotlin.jvm.Synchronized
    @kotlin.jvm.Throws(exceptionClasses = {java.io.IOException.class})
    public final synchronized void start$app_debug(@org.jetbrains.annotations.NotNull
    android.view.SurfaceHolder surfaceHolder) throws java.io.IOException {
    }
    
    /**
     * Closes the camera and stops sending frames to the underlying frame detector.
     *
     *
     * This camera source may be restarted again by calling [.start].
     *
     *
     * Call [.release] instead to completely shut down this camera source and release the
     * resources of the underlying detector.
     */
    @kotlin.jvm.Synchronized
    public final synchronized void stop$app_debug() {
    }
    
    /**
     * Stops the camera and releases the resources of the camera and underlying detector.
     */
    public final void release() {
    }
    
    public final void setFrameProcessor(@org.jetbrains.annotations.NotNull
    org.tensorflow.lite.examples.objectdetection.camera.FrameProcessor processor) {
    }
    
    public final void updateFlashMode(@org.jetbrains.annotations.NotNull
    java.lang.String flashMode) {
    }
    
    /**
     * Opens the camera and applies the user settings.
     *
     * @throws IOException if camera cannot be found or preview cannot be processed.
     */
    @kotlin.jvm.Throws(exceptionClasses = {java.io.IOException.class})
    private final android.hardware.Camera createCamera() throws java.io.IOException {
        return null;
    }
    
    @kotlin.jvm.Throws(exceptionClasses = {java.io.IOException.class})
    private final void setPreviewAndPictureSize(android.hardware.Camera camera, android.hardware.Camera.Parameters parameters) throws java.io.IOException {
    }
    
    /**
     * Calculates the correct rotation for the given camera id and sets the rotation in the
     * parameters. It also sets the camera's display orientation and rotation.
     *
     * @param parameters the camera parameters for which to set the rotation.
     */
    private final void setRotation(android.hardware.Camera camera, android.hardware.Camera.Parameters parameters) {
    }
    
    /**
     * Creates one buffer for the camera preview callback. The size of the buffer is based off of the
     * camera preview size and the format of the camera image.
     *
     * @return a new preview buffer of the appropriate size for the current camera settings.
     */
    private final byte[] createPreviewBuffer(com.google.android.gms.common.images.Size previewSize) {
        return null;
    }
    
    @kotlin.Metadata(mv = {1, 9, 0}, k = 1, xi = 48, d1 = {"\u00004\n\u0002\u0018\u0002\n\u0002\u0010\u0000\n\u0002\b\u0002\n\u0002\u0010\b\n\u0002\b\u0006\n\u0002\u0010\u0007\n\u0000\n\u0002\u0010\u000e\n\u0000\n\u0002\u0010\u0015\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0002\b\u0002\b\u0086\u0003\u0018\u00002\u00020\u0001B\u0007\b\u0002\u00a2\u0006\u0002\u0010\u0002J\u0012\u0010\u000e\u001a\u0004\u0018\u00010\u000f2\u0006\u0010\u0010\u001a\u00020\u0011H\u0002J\u001a\u0010\u0012\u001a\u0004\u0018\u00010\u00132\u0006\u0010\u0010\u001a\u00020\u00112\u0006\u0010\u0014\u001a\u00020\u000bH\u0002R\u000e\u0010\u0003\u001a\u00020\u0004X\u0086T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u0005\u001a\u00020\u0004X\u0082T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u0006\u001a\u00020\u0004X\u0082T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u0007\u001a\u00020\u0004X\u0082T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\b\u001a\u00020\u0004X\u0082T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\t\u001a\u00020\u0004X\u0082T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\n\u001a\u00020\u000bX\u0082T\u00a2\u0006\u0002\n\u0000R\u000e\u0010\f\u001a\u00020\rX\u0082T\u00a2\u0006\u0002\n\u0000\u00a8\u0006\u0015"}, d2 = {"Lorg/tensorflow/lite/examples/objectdetection/camera/CameraSource$Companion;", "", "()V", "CAMERA_FACING_BACK", "", "DEFAULT_REQUESTED_CAMERA_PREVIEW_HEIGHT", "DEFAULT_REQUESTED_CAMERA_PREVIEW_WIDTH", "IMAGE_FORMAT", "MAX_CAMERA_PREVIEW_WIDTH", "MIN_CAMERA_PREVIEW_WIDTH", "REQUESTED_CAMERA_FPS", "", "TAG", "", "selectPreviewFpsRange", "", "camera", "Landroid/hardware/Camera;", "selectSizePair", "Lorg/tensorflow/lite/examples/objectdetection/camera/CameraSizePair;", "displayAspectRatioInLandscape", "app_debug"})
    public static final class Companion {
        
        private Companion() {
            super();
        }
        
        /**
         * Selects the most suitable preview and picture size, given the display aspect ratio in landscape
         * mode.
         *
         *
         * It's firstly trying to pick the one that has closest aspect ratio to display view with its
         * width be in the specified range [[.MIN_CAMERA_PREVIEW_WIDTH], [ ][.MAX_CAMERA_PREVIEW_WIDTH]]. If there're multiple candidates, choose the one having longest
         * width.
         *
         *
         * If the above looking up failed, chooses the one that has the minimum sum of the differences
         * between the desired values and the actual values for width and height.
         *
         *
         * Even though we only need to find the preview size, it's necessary to find both the preview
         * size and the picture size of the camera together, because these need to have the same aspect
         * ratio. On some hardware, if you would only set the preview size, you will get a distorted
         * image.
         *
         * @param camera the camera to select a preview size from
         * @return the selected preview and picture size pair
         */
        private final org.tensorflow.lite.examples.objectdetection.camera.CameraSizePair selectSizePair(android.hardware.Camera camera, float displayAspectRatioInLandscape) {
            return null;
        }
        
        /**
         * Selects the most suitable preview frames per second range.
         *
         * @param camera the camera to select a frames per second range from
         * @return the selected preview frames per second range
         */
        private final int[] selectPreviewFpsRange(android.hardware.Camera camera) {
            return null;
        }
    }
    
    /**
     * This runnable controls access to the underlying receiver, calling it to process frames when
     * available from the camera. This is designed to run detection on frames as fast as possible
     * (i.e., without unnecessary context switching or waiting on the next frame).
     *
     *
     * While detection is running on a frame, new frames may be received from the camera. As these
     * frames come in, the most recent frame is held onto as pending. As soon as detection and its
     * associated processing is done for the previous frame, detection on the mostly recently received
     * frame will immediately start on the same thread.
     */
    @kotlin.Metadata(mv = {1, 9, 0}, k = 1, xi = 48, d1 = {"\u00004\n\u0002\u0018\u0002\n\u0002\u0018\u0002\n\u0002\b\u0002\n\u0002\u0010\u000b\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0018\u0002\n\u0000\n\u0002\u0010\u0002\n\u0002\b\u0004\n\u0002\u0010\u0012\n\u0000\n\u0002\u0018\u0002\n\u0002\b\u0002\b\u0082\u0004\u0018\u00002\u00020\u0001B\u0007\b\u0000\u00a2\u0006\u0002\u0010\u0002J\b\u0010\t\u001a\u00020\nH\u0016J\u0015\u0010\u000b\u001a\u00020\n2\u0006\u0010\u0003\u001a\u00020\u0004H\u0000\u00a2\u0006\u0002\b\fJ\u001d\u0010\r\u001a\u00020\n2\u0006\u0010\u000e\u001a\u00020\u000f2\u0006\u0010\u0010\u001a\u00020\u0011H\u0000\u00a2\u0006\u0002\b\u0012R\u000e\u0010\u0003\u001a\u00020\u0004X\u0082\u000e\u00a2\u0006\u0002\n\u0000R\u000e\u0010\u0005\u001a\u00020\u0006X\u0082\u0004\u00a2\u0006\u0002\n\u0000R\u0010\u0010\u0007\u001a\u0004\u0018\u00010\bX\u0082\u000e\u00a2\u0006\u0002\n\u0000\u00a8\u0006\u0013"}, d2 = {"Lorg/tensorflow/lite/examples/objectdetection/camera/CameraSource$FrameProcessingRunnable;", "Ljava/lang/Runnable;", "(Lorg/tensorflow/lite/examples/objectdetection/camera/CameraSource;)V", "active", "", "lock", "Ljava/lang/Object;", "pendingFrameData", "Ljava/nio/ByteBuffer;", "run", "", "setActive", "setActive$app_debug", "setNextFrame", "data", "", "camera", "Landroid/hardware/Camera;", "setNextFrame$app_debug", "app_debug"})
    final class FrameProcessingRunnable implements java.lang.Runnable {
        @org.jetbrains.annotations.NotNull
        private final java.lang.Object lock = null;
        private boolean active = true;
        @org.jetbrains.annotations.Nullable
        private java.nio.ByteBuffer pendingFrameData;
        
        public FrameProcessingRunnable() {
            super();
        }
        
        /**
         * Marks the runnable as active/not active. Signals any blocked threads to continue.
         */
        public final void setActive$app_debug(boolean active) {
        }
        
        /**
         * Sets the frame data received from the camera. This adds the previous unused frame buffer (if
         * present) back to the camera, and keeps a pending reference to the frame data for future use.
         */
        public final void setNextFrame$app_debug(@org.jetbrains.annotations.NotNull
        byte[] data, @org.jetbrains.annotations.NotNull
        android.hardware.Camera camera) {
        }
        
        /**
         * As long as the processing thread is active, this executes detection on frames continuously.
         * The next pending frame is either immediately available or hasn't been received yet. Once it
         * is available, we transfer the frame info to local variables and run detection on that frame.
         * It immediately loops back for the next frame without pausing.
         *
         *
         * If detection takes longer than the time in between new frames from the camera, this will
         * mean that this loop will run without ever waiting on a frame, avoiding any context switching
         * or frame acquisition time latency.
         *
         *
         * If you find that this is using more CPU than you'd like, you should probably decrease the
         * FPS setting above to allow for some idle time in between frames.
         */
        @java.lang.Override
        public void run() {
        }
    }
}